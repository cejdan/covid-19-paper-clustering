{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv_path = os.path.join(os.path.abspath(os.path.curdir), \"docs\\\\CORD-19-research-challenge\\\\metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51078 entries, 0 to 51077\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   cord_uid                     51078 non-null  object \n",
      " 1   sha                          38022 non-null  object \n",
      " 2   source_x                     51078 non-null  object \n",
      " 3   title                        50920 non-null  object \n",
      " 4   doi                          47741 non-null  object \n",
      " 5   pmcid                        41082 non-null  object \n",
      " 6   pubmed_id                    37861 non-null  float64\n",
      " 7   license                      51078 non-null  object \n",
      " 8   abstract                     42352 non-null  object \n",
      " 9   publish_time                 51070 non-null  object \n",
      " 10  authors                      48891 non-null  object \n",
      " 11  journal                      46368 non-null  object \n",
      " 12  Microsoft Academic Paper ID  964 non-null    float64\n",
      " 13  WHO #Covidence               1768 non-null   object \n",
      " 14  has_pdf_parse                51078 non-null  bool   \n",
      " 15  has_pmc_xml_parse            51078 non-null  bool   \n",
      " 16  full_text_file               42511 non-null  object \n",
      " 17  url                          50776 non-null  object \n",
      "dtypes: bool(2), float64(2), object(14)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "metaCSV = pd.read_csv(metadata_csv_path)\n",
    "metaCSV.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaCSV = metaCSV[['cord_uid', 'sha', 'pmcid', 'title', 'abstract', 'authors', 'journal']]\n",
    "#metaCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the items with titles, abstracts, and sha values.\n",
    "metaCSV.dropna(inplace=True, subset = {'title', 'abstract', 'sha'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nic\\\\PycharmProjects\\\\cs5293sp20-project2\\\\docs\\\\CORD-19-research-challenge\\\\comm_use_subset\\\\comm_use_subset\\\\pdf_json\\\\5b68a553a7cbbea13472721cd1ad617d42b40c26.json'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = metaCSV.iloc[3,1] + \".json\"\n",
    "filepath = \"docs\\\\CORD-19-research-challenge\\\\comm_use_subset\\\\comm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "os.path.abspath(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block will create a boolean filter to remove all the rows that are not in the comm_use_subset\n",
    "#But we will use the more comprehensive dataset instead.\n",
    "\n",
    "#i = 0\n",
    "#in_subset = []\n",
    "#for items in metaCSV['sha']:\n",
    "#    filename = metaCSV.iloc[i,1] + \".json\"\n",
    "#    filepath = \"docs\\\\CORD-19-research-challenge\\\\comm_use_subset\\\\comm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "#    os.path.abspath(filepath)\n",
    "#    in_subset.append(os.path.exists(os.path.abspath(filepath)))\n",
    "#    i = i + 1\n",
    "#pd.Series(in_subset)\n",
    "#in_subset\n",
    "#meta_CSV = metaCSV[in_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "         ... \n",
       "33531    True\n",
       "33532    True\n",
       "33533    True\n",
       "33534    True\n",
       "33535    True\n",
       "Length: 33536, dtype: bool"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_count = 0\n",
    "in_comm_use = []\n",
    "in_noncomm_use = []\n",
    "in_custom_license = []\n",
    "in_biorxiv_medrxiv = []\n",
    "\n",
    "in_any_subset = []\n",
    "\n",
    "for items in metaCSV['sha']:\n",
    "    filename = metaCSV.iloc[loop_count,1] + \".json\"\n",
    "    filepath1 = \"docs\\\\CORD-19-research-challenge\\\\comm_use_subset\\\\comm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "    filepath2 = \"docs\\\\CORD-19-research-challenge\\\\noncomm_use_subset\\\\noncomm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "    filepath3 = \"docs\\\\CORD-19-research-challenge\\\\custom_license\\\\custom_license\\\\pdf_json\\\\\" + filename\n",
    "    filepath4 = \"docs\\\\CORD-19-research-challenge\\\\biorxiv_medrxiv\\\\biorxiv_medrxiv\\\\pdf_json\\\\\" + filename\n",
    "\n",
    "    in_comm_use.append(os.path.exists(os.path.abspath(filepath1)))\n",
    "    in_noncomm_use.append(os.path.exists(os.path.abspath(filepath2)))\n",
    "    in_custom_license.append(os.path.exists(os.path.abspath(filepath3)))\n",
    "    in_biorxiv_medrxiv.append(os.path.exists(os.path.abspath(filepath4)))\n",
    "    \n",
    "    \n",
    "    if in_comm_use[loop_count] == True or in_noncomm_use[loop_count] == True or in_custom_license[loop_count] == True or in_biorxiv_medrxiv[loop_count] == True:\n",
    "        in_any_subset.append(True)\n",
    "    else:\n",
    "        in_any_subset.append(False)\n",
    "    loop_count = loop_count + 1\n",
    "pd.Series(in_any_subset)\n",
    "#in_any_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31646 entries, 0 to 51076\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   cord_uid  31646 non-null  object\n",
      " 1   sha       31646 non-null  object\n",
      " 2   pmcid     26925 non-null  object\n",
      " 3   title     31646 non-null  object\n",
      " 4   abstract  31646 non-null  object\n",
      " 5   authors   31487 non-null  object\n",
      " 6   journal   29928 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "meta_CSV = metaCSV[in_any_subset]\n",
    "meta_CSV.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13761</th>\n",
       "      <td>3zturry3</td>\n",
       "      <td>dd63c8a6ff3ae72819472ca9eef5dfb0f13afb5e</td>\n",
       "      <td>PMC7089353</td>\n",
       "      <td>High-level expression of a human β-site APP cl...</td>\n",
       "      <td>Plastid transformation has to date been applie...</td>\n",
       "      <td>Youm, Jung Won; Jeon, Jae Heung; Kim, Hee; Min...</td>\n",
       "      <td>Transgenic Res</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12425</th>\n",
       "      <td>yi2l5bcu</td>\n",
       "      <td>7e4c7d38872c37923d5ba6e8da2c6b01b72f176d</td>\n",
       "      <td>PMC7080100</td>\n",
       "      <td>Review of plasmonic fiber optic biochemical se...</td>\n",
       "      <td>This paper presents a brief overview of the te...</td>\n",
       "      <td>Caucheteur, Christophe; Guo, Tuan; Albert, Jac...</td>\n",
       "      <td>Anal Bioanal Chem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>pd9lt4n2</td>\n",
       "      <td>dd969a7721176db21314930b62e62a37e902dae5</td>\n",
       "      <td>PMC4453572</td>\n",
       "      <td>Heparan Sulfate-Dependent Enhancement of Henip...</td>\n",
       "      <td>Nipah virus and Hendra virus are emerging, hig...</td>\n",
       "      <td>Mathieu, Cyrille; Dhondt, Kévin P.; Châlons, M...</td>\n",
       "      <td>mBio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>r46d5kdu</td>\n",
       "      <td>33310fc6d99fa6bbf3aa6160168f3acba6a736d7</td>\n",
       "      <td>PMC4728157</td>\n",
       "      <td>Roles of the hemagglutinin of influenza A viru...</td>\n",
       "      <td>Seasonal influenza epidemics and influenza pan...</td>\n",
       "      <td>Jiang, Shibo; Li, Runming; Du, Lanying; Liu, S...</td>\n",
       "      <td>Protein &amp; Cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50818</th>\n",
       "      <td>e93nu56p</td>\n",
       "      <td>adec4c54ca0073999de9e63c5806585d5ac2c358</td>\n",
       "      <td>PMC7116949</td>\n",
       "      <td>The ventilation of multiple-bed hospital wards...</td>\n",
       "      <td>Abstract Hospital and healthcare facilities ha...</td>\n",
       "      <td>Yau, Y.H.; Chandrasegaran, D.; Badarudin, A.</td>\n",
       "      <td>Building and Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49765</th>\n",
       "      <td>iwv9lilo</td>\n",
       "      <td>1e90e5ac4f30837179cd3ff9f9f380cd4dcaeca8</td>\n",
       "      <td>PMC7117285</td>\n",
       "      <td>Characterization of turkey coronavirus from tu...</td>\n",
       "      <td>Abstract The present study was to characterize...</td>\n",
       "      <td>Lin, Tsang L.; Loa, Chien C.; Tsai, Shih C.; W...</td>\n",
       "      <td>Veterinary Microbiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>jngjogrh</td>\n",
       "      <td>67de735e7e3f2e60bfd376d42819b00faa8bd95e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Disidratazione acuta nel lattante</td>\n",
       "      <td>Il lattante al di sotto di un anno, e soprattu...</td>\n",
       "      <td>Hubert, P.</td>\n",
       "      <td>EMC - Urgenze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12060</th>\n",
       "      <td>m128nfhn</td>\n",
       "      <td>95cd65dbf317e24c13557dbf46ffbaa4c206f8be</td>\n",
       "      <td>PMC7019308</td>\n",
       "      <td>Isolation and Identification of Porcine Deltac...</td>\n",
       "      <td>Porcine deltacoronavirus (PDCoV) is a porcine ...</td>\n",
       "      <td>Qian, Shaoju; Jia, Xiangchao; Gao, Zitong; Zha...</td>\n",
       "      <td>Viruses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50715</th>\n",
       "      <td>r1vsvdz0</td>\n",
       "      <td>4a817cc21c19f64d48b5e0a9e062b934eec46955</td>\n",
       "      <td>PMC7126289</td>\n",
       "      <td>A novel method for making human monoclonal ant...</td>\n",
       "      <td>Abstract We have developed a B cell immortaliz...</td>\n",
       "      <td>Fraussen, J.; Vrolix, K.; Martinez-Martinez, P...</td>\n",
       "      <td>Journal of Autoimmunity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48650</th>\n",
       "      <td>k7ujru6t</td>\n",
       "      <td>b8f9525f63118ab787c2f81dc9fff88cba0eecc7</td>\n",
       "      <td>PMC7134557</td>\n",
       "      <td>Identification and functional characterization...</td>\n",
       "      <td>Abstract The helicase-like domain of the Bambo...</td>\n",
       "      <td>Han, Yu-Tsung; Hsu, Yau-Heiu; Lo, Chia-Wen; Me...</td>\n",
       "      <td>Virology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cord_uid                                       sha       pmcid  \\\n",
       "13761  3zturry3  dd63c8a6ff3ae72819472ca9eef5dfb0f13afb5e  PMC7089353   \n",
       "12425  yi2l5bcu  7e4c7d38872c37923d5ba6e8da2c6b01b72f176d  PMC7080100   \n",
       "5169   pd9lt4n2  dd969a7721176db21314930b62e62a37e902dae5  PMC4453572   \n",
       "5971   r46d5kdu  33310fc6d99fa6bbf3aa6160168f3acba6a736d7  PMC4728157   \n",
       "50818  e93nu56p  adec4c54ca0073999de9e63c5806585d5ac2c358  PMC7116949   \n",
       "...         ...                                       ...         ...   \n",
       "49765  iwv9lilo  1e90e5ac4f30837179cd3ff9f9f380cd4dcaeca8  PMC7117285   \n",
       "31306  jngjogrh  67de735e7e3f2e60bfd376d42819b00faa8bd95e         NaN   \n",
       "12060  m128nfhn  95cd65dbf317e24c13557dbf46ffbaa4c206f8be  PMC7019308   \n",
       "50715  r1vsvdz0  4a817cc21c19f64d48b5e0a9e062b934eec46955  PMC7126289   \n",
       "48650  k7ujru6t  b8f9525f63118ab787c2f81dc9fff88cba0eecc7  PMC7134557   \n",
       "\n",
       "                                                   title  \\\n",
       "13761  High-level expression of a human β-site APP cl...   \n",
       "12425  Review of plasmonic fiber optic biochemical se...   \n",
       "5169   Heparan Sulfate-Dependent Enhancement of Henip...   \n",
       "5971   Roles of the hemagglutinin of influenza A viru...   \n",
       "50818  The ventilation of multiple-bed hospital wards...   \n",
       "...                                                  ...   \n",
       "49765  Characterization of turkey coronavirus from tu...   \n",
       "31306                  Disidratazione acuta nel lattante   \n",
       "12060  Isolation and Identification of Porcine Deltac...   \n",
       "50715  A novel method for making human monoclonal ant...   \n",
       "48650  Identification and functional characterization...   \n",
       "\n",
       "                                                abstract  \\\n",
       "13761  Plastid transformation has to date been applie...   \n",
       "12425  This paper presents a brief overview of the te...   \n",
       "5169   Nipah virus and Hendra virus are emerging, hig...   \n",
       "5971   Seasonal influenza epidemics and influenza pan...   \n",
       "50818  Abstract Hospital and healthcare facilities ha...   \n",
       "...                                                  ...   \n",
       "49765  Abstract The present study was to characterize...   \n",
       "31306  Il lattante al di sotto di un anno, e soprattu...   \n",
       "12060  Porcine deltacoronavirus (PDCoV) is a porcine ...   \n",
       "50715  Abstract We have developed a B cell immortaliz...   \n",
       "48650  Abstract The helicase-like domain of the Bambo...   \n",
       "\n",
       "                                                 authors  \\\n",
       "13761  Youm, Jung Won; Jeon, Jae Heung; Kim, Hee; Min...   \n",
       "12425  Caucheteur, Christophe; Guo, Tuan; Albert, Jac...   \n",
       "5169   Mathieu, Cyrille; Dhondt, Kévin P.; Châlons, M...   \n",
       "5971   Jiang, Shibo; Li, Runming; Du, Lanying; Liu, S...   \n",
       "50818       Yau, Y.H.; Chandrasegaran, D.; Badarudin, A.   \n",
       "...                                                  ...   \n",
       "49765  Lin, Tsang L.; Loa, Chien C.; Tsai, Shih C.; W...   \n",
       "31306                                         Hubert, P.   \n",
       "12060  Qian, Shaoju; Jia, Xiangchao; Gao, Zitong; Zha...   \n",
       "50715  Fraussen, J.; Vrolix, K.; Martinez-Martinez, P...   \n",
       "48650  Han, Yu-Tsung; Hsu, Yau-Heiu; Lo, Chia-Wen; Me...   \n",
       "\n",
       "                        journal  \n",
       "13761            Transgenic Res  \n",
       "12425         Anal Bioanal Chem  \n",
       "5169                       mBio  \n",
       "5971             Protein & Cell  \n",
       "50818  Building and Environment  \n",
       "...                         ...  \n",
       "49765   Veterinary Microbiology  \n",
       "31306             EMC - Urgenze  \n",
       "12060                   Viruses  \n",
       "50715   Journal of Autoimmunity  \n",
       "48650                  Virology  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 9,524 items in the comm_use_subset pdf list\n",
    "#There are 9,148 items in the comm_use_subset pmc list\n",
    "#There are 2,490 items in the noncomm_use_subset pdf list\n",
    "#There are 2,217 items in the noncomm_use_subset pmc list\n",
    "#There are 26,505 items in the custom_license pdf list\n",
    "#There are 7,802 items in the custom_license pmc list\n",
    "#There are 1,625 items in the biorxiv_medrxiv pdf list\n",
    "\n",
    "#That's a total of 40,144 pdf papers (with sha names)\n",
    "#Using the sha code, we found 31,646 papers.\n",
    "#So, 8,498 items were not found with the sha code, that's fine.\n",
    "\n",
    "#We can now take a random sample of these 31,000 papers to generate a much smaller list of 5000.\n",
    "\n",
    "meta_sample = meta_CSV.sample(n=5000, random_state = 666) #because it's funny\n",
    "meta_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs\\\\CORD-19-research-challenge\\\\custom_license\\\\custom_license\\\\pdf_json\\\\dd63c8a6ff3ae72819472ca9eef5dfb0f13afb5e.json',\n",
       " 'docs\\\\CORD-19-research-challenge\\\\custom_license\\\\custom_license\\\\pdf_json\\\\7e4c7d38872c37923d5ba6e8da2c6b01b72f176d.json']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = []\n",
    "\n",
    "for i in range (0,len(meta_sample)):\n",
    "    sha = meta_sample.iloc[i,1] #meta_sample.iloc[i,1]\n",
    "    filename = sha + \".json\"\n",
    "    \n",
    "    filepath1 = \"docs\\\\CORD-19-research-challenge\\\\comm_use_subset\\\\comm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "    filepath2 = \"docs\\\\CORD-19-research-challenge\\\\noncomm_use_subset\\\\noncomm_use_subset\\\\pdf_json\\\\\" + filename\n",
    "    filepath3 = \"docs\\\\CORD-19-research-challenge\\\\custom_license\\\\custom_license\\\\pdf_json\\\\\" + filename\n",
    "    filepath4 = \"docs\\\\CORD-19-research-challenge\\\\biorxiv_medrxiv\\\\biorxiv_medrxiv\\\\pdf_json\\\\\" + filename\n",
    "    \n",
    "    if os.path.exists(os.path.abspath(filepath1)):\n",
    "        filepaths.append(filepath1)\n",
    "    elif os.path.exists(os.path.abspath(filepath2)):\n",
    "        filepaths.append(filepath2)\n",
    "    elif os.path.exists(os.path.abspath(filepath3)):\n",
    "        filepaths.append(filepath3)\n",
    "    elif os.path.exists(os.path.abspath(filepath4)):\n",
    "        filepaths.append(filepath4)\n",
    "    \n",
    "    \n",
    "#filepaths[:2]\n",
    "    \n",
    "    #globstring = os.path.abspath(os.curdir) + '\\\\docs\\\\CORD-19-research-challenge\\\\**\\\\pdf_json\\\\' + filename\n",
    "    #filepath = glob.glob(globstring, recursive = True)\n",
    "    #filepaths.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0 processed\n",
      "text 500 processed\n",
      "text 1000 processed\n",
      "text 1500 processed\n",
      "text 2000 processed\n",
      "text 2500 processed\n",
      "text 3000 processed\n",
      "text 3500 processed\n",
      "text 4000 processed\n",
      "text 4500 processed\n",
      "text 4999 processed\n"
     ]
    }
   ],
   "source": [
    "#Excellent. Now we need to read in the .json files in a way that allows us to access the text, so we can tokenize it.\n",
    "\n",
    "\n",
    "full_text_list = []\n",
    "\n",
    "for i in range (0,len(filepaths)):\n",
    "\n",
    "    with open(filepaths[i], 'r') as file:\n",
    "        myjson = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    full_text = \"\"\n",
    "    for x in range(0,len(myjson['body_text'])):\n",
    "        if x == len(myjson['body_text'])-1:\n",
    "            full_text = full_text + myjson['body_text'][x][\"text\"]\n",
    "        else:      \n",
    "            full_text = full_text + myjson['body_text'][x][\"text\"] + \" \"\n",
    "    \n",
    "    #Just some helpful progress text since this block takes a few seconds.\n",
    "    if i % 500 == 0:\n",
    "        print(\"text \" + str(i) + \" processed\")\n",
    "    if i == len(filepaths) - 1:\n",
    "        print(\"text \" + str(i) + \" processed\")\n",
    "\n",
    "    full_text_list.append(full_text)\n",
    "\n",
    "#full_text_list.head()\n",
    "#I will actually append this full text to a monster list.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#The goal is to create a dataframe in the end, which we can work with more easily.\n",
    "#My vision is something like:\n",
    "#    sha/paper_id     title              abstract_text (list of tokens)          body_text (list of tokens)          TF-IDF vector\n",
    "#    1200128asfd34    High-level...      'We found that ...'    '[Introduction, Planarians, posess, ability, ...]     [0.5,0.2,0.3,...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0 processed\n",
      "text 500 processed\n",
      "text 1000 processed\n",
      "text 1500 processed\n",
      "text 2000 processed\n",
      "text 2500 processed\n",
      "text 3000 processed\n",
      "text 3500 processed\n",
      "text 4000 processed\n",
      "text 4500 processed\n",
      "text 4999 processed\n"
     ]
    }
   ],
   "source": [
    "full_abstract_list = []\n",
    "\n",
    "for i in range (0,len(filepaths)):\n",
    "\n",
    "    with open(filepaths[i], 'r') as file:\n",
    "        myjson = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    full_abstract_text = \"\"\n",
    "    for x in range(0,len(myjson['abstract'])):\n",
    "        if x == len(myjson['abstract'])-1:\n",
    "            full_abstract_text = full_abstract_text + myjson['abstract'][x][\"text\"]\n",
    "        else:      \n",
    "            full_abstract_text = full_abstract_text + myjson['abstract'][x][\"text\"] + \" \"\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(\"text \" + str(i) + \" processed\")\n",
    "    elif i == len(filepaths) - 1:\n",
    "        print(\"text \" + str(i) + \" processed\")\n",
    "        \n",
    "    full_abstract_list.append(full_abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0 tokenized (0.0% complete)\n",
      "text 100 tokenized (2.0% complete)\n",
      "text 200 tokenized (4.0% complete)\n",
      "text 300 tokenized (6.0% complete)\n",
      "text 400 tokenized (8.0% complete)\n",
      "text 500 tokenized (10.0% complete)\n",
      "text 600 tokenized (12.0% complete)\n",
      "text 700 tokenized (14.000000000000002% complete)\n",
      "text 800 tokenized (16.0% complete)\n",
      "text 900 tokenized (18.0% complete)\n",
      "text 1000 tokenized (20.0% complete)\n",
      "text 1100 tokenized (22.0% complete)\n",
      "text 1200 tokenized (24.0% complete)\n",
      "text 1300 tokenized (26.0% complete)\n",
      "text 1400 tokenized (28.000000000000004% complete)\n",
      "text 1500 tokenized (30.0% complete)\n",
      "text 1600 tokenized (32.0% complete)\n",
      "text 1700 tokenized (34.0% complete)\n",
      "text 1800 tokenized (36.0% complete)\n",
      "text 1900 tokenized (38.0% complete)\n",
      "text 2000 tokenized (40.0% complete)\n",
      "text 2100 tokenized (42.0% complete)\n",
      "text 2200 tokenized (44.0% complete)\n",
      "text 2300 tokenized (46.0% complete)\n",
      "text 2400 tokenized (48.0% complete)\n",
      "text 2500 tokenized (50.0% complete)\n",
      "text 2600 tokenized (52.0% complete)\n",
      "text 2700 tokenized (54.0% complete)\n",
      "text 2800 tokenized (56.00000000000001% complete)\n",
      "text 2900 tokenized (57.99999999999999% complete)\n",
      "text 3000 tokenized (60.0% complete)\n",
      "text 3100 tokenized (62.0% complete)\n",
      "text 3200 tokenized (64.0% complete)\n",
      "text 3300 tokenized (66.0% complete)\n",
      "text 3400 tokenized (68.0% complete)\n",
      "text 3500 tokenized (70.0% complete)\n",
      "text 3600 tokenized (72.0% complete)\n",
      "text 3700 tokenized (74.0% complete)\n",
      "text 3800 tokenized (76.0% complete)\n",
      "text 3900 tokenized (78.0% complete)\n",
      "text 4000 tokenized (80.0% complete)\n",
      "text 4100 tokenized (82.0% complete)\n",
      "text 4200 tokenized (84.0% complete)\n",
      "text 4300 tokenized (86.0% complete)\n",
      "text 4400 tokenized (88.0% complete)\n",
      "text 4500 tokenized (90.0% complete)\n",
      "text 4600 tokenized (92.0% complete)\n",
      "text 4700 tokenized (94.0% complete)\n",
      "text 4800 tokenized (96.0% complete)\n",
      "text 4900 tokenized (98.0% complete)\n",
      "text 4999 tokenized (100% complete)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is where we can actually start doing some Natural Language Processing.\n",
    "#Step 1 is always to clean up our text. It is a still kinda a hot mess, lots of extra symbols, tons of words which cloud the\n",
    "#meaning of the text (called stopwords, \"the\", \"we\", \"a\", etc. stuff like that)\n",
    "\n",
    "\n",
    "#nltk.download('stopwords') need this for the first run.\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "extraStopWords = ['et','al', 'al.', '(', ')', ',', '.', ':', ';', '%', '#', '@', '&', '!', '?', '[', ']']\n",
    "stop_words.extend(extraStopWords)\n",
    "\n",
    "tokenized_text = []\n",
    "\n",
    "def normalizeDoc(txt):\n",
    "    for i in range(0,len(txt)):\n",
    "        myText = txt[i]\n",
    "        myText = re.sub(r'[^a-zA-Z0-9\\s]', '', myText, re.I|re.A)\n",
    "        myText = myText.lower()\n",
    "        myText = myText.strip()\n",
    "        tokens = nltk.word_tokenize(myText)\n",
    "        clean_tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokenized_text.append(clean_tokens)\n",
    "        if i % 100 == 0:\n",
    "            print(\"text \" + str(i) + \" tokenized (\"+ str(i/len(txt)*100) + \"% complete)\")\n",
    "        elif i == len(txt) - 1:\n",
    "            print(\"text \" + str(i) + \" tokenized (100% complete)\")\n",
    "            \n",
    "normalizeDoc(full_text_list)\n",
    "len(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['influenza',\n",
       "  'continues',\n",
       "  'pose',\n",
       "  'serious',\n",
       "  'threats',\n",
       "  'public',\n",
       "  'health',\n",
       "  'worldwide',\n",
       "  'since',\n",
       "  'seasonal',\n",
       "  'influenza',\n",
       "  'epidemics',\n",
       "  'affect',\n",
       "  '15',\n",
       "  'population',\n",
       "  'result',\n",
       "  '500000',\n",
       "  'deaths',\n",
       "  'worldwide',\n",
       "  'year',\n",
       "  'httpwwwwhointcsrdisease',\n",
       "  'influenzaresearchagendadocumentpdf',\n",
       "  'influenza',\n",
       "  'pandemics',\n",
       "  'like',\n",
       "  '1918',\n",
       "  'flu',\n",
       "  'pandemic',\n",
       "  'spanish',\n",
       "  'flu',\n",
       "  'may',\n",
       "  'lead',\n",
       "  'millions',\n",
       "  'deaths',\n",
       "  'taubenberger',\n",
       "  '2006',\n",
       "  '2009',\n",
       "  'new',\n",
       "  'influenza',\n",
       "  'pandemic',\n",
       "  'caused',\n",
       "  'novel',\n",
       "  'swineorigin',\n",
       "  'influenza',\n",
       "  'virus',\n",
       "  'soiv',\n",
       "  'h1n1',\n",
       "  'resulted',\n",
       "  'millions',\n",
       "  'infections',\n",
       "  '213',\n",
       "  'countries',\n",
       "  'overseas',\n",
       "  'territories',\n",
       "  'communities',\n",
       "  'httpwwwwhointcsrdon',\n",
       "  '20100312enindexhtml',\n",
       "  'miller',\n",
       "  '2009',\n",
       "  'smith',\n",
       "  '2009',\n",
       "  'itoh',\n",
       "  '2009',\n",
       "  'rapid',\n",
       "  'spread',\n",
       "  'soiv',\n",
       "  'humans',\n",
       "  'worldwide',\n",
       "  'continuous',\n",
       "  'mutations',\n",
       "  'soiv',\n",
       "  'proteins',\n",
       "  'pan',\n",
       "  'jiang',\n",
       "  '2009',\n",
       "  'pan',\n",
       "  '2010',\n",
       "  'suggests',\n",
       "  'newly',\n",
       "  'emerging',\n",
       "  'animalorigin',\n",
       "  'virus',\n",
       "  'acquired',\n",
       "  'potent',\n",
       "  'humantohuman',\n",
       "  'transmissibility',\n",
       "  'raised',\n",
       "  'growing',\n",
       "  'concern',\n",
       "  'evolution',\n",
       "  'highly',\n",
       "  'pathogenic',\n",
       "  'avian',\n",
       "  'influenza',\n",
       "  'hpai',\n",
       "  'h5n1',\n",
       "  'viruses',\n",
       "  'caused',\n",
       "  '289',\n",
       "  'fatal',\n",
       "  'cases',\n",
       "  'among',\n",
       "  'total',\n",
       "  '489',\n",
       "  'infected',\n",
       "  'individuals',\n",
       "  'march',\n",
       "  '16',\n",
       "  '2010',\n",
       "  'casefatality',\n",
       "  'rate',\n",
       "  '60',\n",
       "  'httpwwwwhointcsrdisease',\n",
       "  'avianinfluenzacountrycasestable20100316enindexhtml',\n",
       "  'real',\n",
       "  'h5n1',\n",
       "  'mortality',\n",
       "  'rate',\n",
       "  'closer',\n",
       "  '1433',\n",
       "  'based',\n",
       "  'surveillance',\n",
       "  'seroprevalence',\n",
       "  'studies',\n",
       "  'conducted',\n",
       "  'several',\n",
       "  'countries',\n",
       "  'li',\n",
       "  '2008',\n",
       "  'influenza',\n",
       "  'virus',\n",
       "  'consists',\n",
       "  'three',\n",
       "  'genera',\n",
       "  'b',\n",
       "  'c',\n",
       "  'although',\n",
       "  'three',\n",
       "  'types',\n",
       "  'influenza',\n",
       "  'viruses',\n",
       "  'able',\n",
       "  'infect',\n",
       "  'humans',\n",
       "  'influenza',\n",
       "  'virus',\n",
       "  'iav',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'influenza',\n",
       "  'pandemics',\n",
       "  'watts',\n",
       "  '2009',\n",
       "  'based',\n",
       "  'antigenicity',\n",
       "  'two',\n",
       "  'surface',\n",
       "  'proteinshemagglutinin',\n",
       "  'ha',\n",
       "  'neuraminidase',\n",
       "  'na',\n",
       "  'iav',\n",
       "  'classified',\n",
       "  'different',\n",
       "  'subtypes',\n",
       "  'including',\n",
       "  '16',\n",
       "  'ha',\n",
       "  'h1h16',\n",
       "  '9',\n",
       "  'na',\n",
       "  'n1n9',\n",
       "  'subtypes',\n",
       "  'yamashita',\n",
       "  '2010',\n",
       "  'ha',\n",
       "  'responsible',\n",
       "  'viral',\n",
       "  'entry',\n",
       "  'russell',\n",
       "  '2008',\n",
       "  'contains',\n",
       "  'major',\n",
       "  'neutralizing',\n",
       "  'epitopes',\n",
       "  'ndifon',\n",
       "  '2009',\n",
       "  'thus',\n",
       "  'serving',\n",
       "  'attractive',\n",
       "  'target',\n",
       "  'drug',\n",
       "  'vaccine',\n",
       "  'development',\n",
       "  'transferred',\n",
       "  'cell',\n",
       "  'surface',\n",
       "  'golgi',\n",
       "  'apparatus',\n",
       "  'ha0',\n",
       "  'proteolytically',\n",
       "  'cleaved',\n",
       "  'functional',\n",
       "  'ha1',\n",
       "  'ha2',\n",
       "  'subunits',\n",
       "  'linked',\n",
       "  'single',\n",
       "  'disulfide',\n",
       "  'bond',\n",
       "  'influenza',\n",
       "  'virus',\n",
       "  'strains',\n",
       "  'contain',\n",
       "  'ha',\n",
       "  'cleavage',\n",
       "  'site',\n",
       "  'single',\n",
       "  'basic',\n",
       "  'amino',\n",
       "  'acid',\n",
       "  'residue',\n",
       "  'cleaved',\n",
       "  'tissue',\n",
       "  'restricted',\n",
       "  'proteases',\n",
       "  'thereby',\n",
       "  'limiting',\n",
       "  'spread',\n",
       "  'infected',\n",
       "  'host',\n",
       "  'cleavage',\n",
       "  'occurs',\n",
       "  'cell',\n",
       "  'surface',\n",
       "  'released',\n",
       "  'viruses',\n",
       "  'ha0',\n",
       "  'h1',\n",
       "  'h2',\n",
       "  'h3',\n",
       "  'subtype',\n",
       "  'viruses',\n",
       "  'caused',\n",
       "  'epidemics',\n",
       "  'cleavage',\n",
       "  'may',\n",
       "  'mediated',\n",
       "  'serine',\n",
       "  'protease',\n",
       "  'tryptase',\n",
       "  'clara',\n",
       "  'produced',\n",
       "  'clara',\n",
       "  'cells',\n",
       "  'bronchiolar',\n",
       "  'epithelium',\n",
       "  'kido',\n",
       "  '1992',\n",
       "  'enzyme',\n",
       "  'shows',\n",
       "  'recognition',\n",
       "  'specificity',\n",
       "  'sequence',\n",
       "  'qexr',\n",
       "  'found',\n",
       "  'cleavage',\n",
       "  'sites',\n",
       "  'contrast',\n",
       "  'h5',\n",
       "  'h7',\n",
       "  'hpai',\n",
       "  'subtypes',\n",
       "  'ha1',\n",
       "  'ha2',\n",
       "  'polypeptide',\n",
       "  'chains',\n",
       "  'separated',\n",
       "  'polybasic',\n",
       "  'sequences',\n",
       "  'inserted',\n",
       "  'cleavage',\n",
       "  'site',\n",
       "  'perdue',\n",
       "  '1997',\n",
       "  'cases',\n",
       "  'cleavage',\n",
       "  'intracellular',\n",
       "  'involves',\n",
       "  'subtilisinlike',\n",
       "  'enzymes',\n",
       "  'active',\n",
       "  'posttranslational',\n",
       "  'processing',\n",
       "  'hormone',\n",
       "  'growth',\n",
       "  'factor',\n",
       "  'precursors',\n",
       "  'stienekegrber',\n",
       "  '1992',\n",
       "  'furin',\n",
       "  'recognition',\n",
       "  'sequence',\n",
       "  'rxrkr',\n",
       "  'common',\n",
       "  'feature',\n",
       "  'inserted',\n",
       "  'polybasic',\n",
       "  'sequences',\n",
       "  'wide',\n",
       "  'tissue',\n",
       "  'distribution',\n",
       "  'furinlike',\n",
       "  'enzymes',\n",
       "  'high',\n",
       "  'efficiency',\n",
       "  'intracellular',\n",
       "  'cleavage',\n",
       "  'compared',\n",
       "  'extracellular',\n",
       "  'cleavage',\n",
       "  'appear',\n",
       "  'related',\n",
       "  'widespread',\n",
       "  'systemic',\n",
       "  'virulent',\n",
       "  'infections',\n",
       "  'caused',\n",
       "  'h5',\n",
       "  'h7',\n",
       "  'viruses',\n",
       "  'birds',\n",
       "  'localized',\n",
       "  'outbreak',\n",
       "  'h5n1',\n",
       "  'infection',\n",
       "  'humans',\n",
       "  'hong',\n",
       "  'kong',\n",
       "  '1997',\n",
       "  'steinhauer',\n",
       "  '1999',\n",
       "  'receptors',\n",
       "  'iav',\n",
       "  'sialic',\n",
       "  'acids',\n",
       "  'weis',\n",
       "  '1988',\n",
       "  'bullough',\n",
       "  '1994',\n",
       "  'liu',\n",
       "  '2009',\n",
       "  'depending',\n",
       "  'species',\n",
       "  'infected',\n",
       "  'receptor',\n",
       "  'recognition',\n",
       "  'varies',\n",
       "  'specificity',\n",
       "  'nature',\n",
       "  'glycosidic',\n",
       "  'linkage',\n",
       "  'sialic',\n",
       "  'acid',\n",
       "  'penultimate',\n",
       "  'sugar',\n",
       "  'sidechains',\n",
       "  'rogers',\n",
       "  'paulson',\n",
       "  '1983',\n",
       "  'example',\n",
       "  'human',\n",
       "  'iavs',\n",
       "  'bind',\n",
       "  'sialic',\n",
       "  'acid',\n",
       "  '26linkage',\n",
       "  'predominant',\n",
       "  'upper',\n",
       "  'human',\n",
       "  'respiratory',\n",
       "  'tracts',\n",
       "  'matrosovich',\n",
       "  '2004',\n",
       "  'whereas',\n",
       "  'avian',\n",
       "  'iavs',\n",
       "  'prefer',\n",
       "  'sialic',\n",
       "  'acid',\n",
       "  '23linkage',\n",
       "  'galactose',\n",
       "  'predominates',\n",
       "  'avian',\n",
       "  'enteric',\n",
       "  'tract',\n",
       "  'naeve',\n",
       "  '1984',\n",
       "  'fig',\n",
       "  '1',\n",
       "  'therefore',\n",
       "  'ha',\n",
       "  'key',\n",
       "  'component',\n",
       "  'animal',\n",
       "  'iav',\n",
       "  'become',\n",
       "  'transmissible',\n",
       "  'humans',\n",
       "  'instance',\n",
       "  'h5n1',\n",
       "  'iav',\n",
       "  'may',\n",
       "  'acquire',\n",
       "  'humantohuman',\n",
       "  'transmissibility',\n",
       "  'ha',\n",
       "  'gains',\n",
       "  'ability',\n",
       "  'bind',\n",
       "  '26linked',\n",
       "  'receptors',\n",
       "  'stevens',\n",
       "  '2006',\n",
       "  'xu',\n",
       "  '2010',\n",
       "  'binding',\n",
       "  'receptor',\n",
       "  'virus',\n",
       "  'taken',\n",
       "  'cells',\n",
       "  'endocytosis',\n",
       "  'within',\n",
       "  'endosomal',\n",
       "  'compartment',\n",
       "  'virion',\n",
       "  'exposed',\n",
       "  'increasing',\n",
       "  'acid',\n",
       "  'ph',\n",
       "  '56',\n",
       "  'ha',\n",
       "  'protein',\n",
       "  'undergoes',\n",
       "  'irreversible',\n",
       "  'conformation',\n",
       "  'change',\n",
       "  'metastable',\n",
       "  'prefusion',\n",
       "  'conformation',\n",
       "  'lowph',\n",
       "  'hairpin',\n",
       "  'structure',\n",
       "  'involving',\n",
       "  'extrusion',\n",
       "  'fusion',\n",
       "  'peptide',\n",
       "  'fp',\n",
       "  'interior',\n",
       "  'ha2',\n",
       "  'neutralph',\n",
       "  'structure',\n",
       "  'toward',\n",
       "  'endosomal',\n",
       "  'membrane',\n",
       "  'promoting',\n",
       "  'fusion',\n",
       "  'viral',\n",
       "  'endosomal',\n",
       "  'membranes',\n",
       "  'harrison',\n",
       "  '2008',\n",
       "  'reed',\n",
       "  '2010',\n",
       "  'xray',\n",
       "  'crystallographic',\n",
       "  'studies',\n",
       "  'demonstrated',\n",
       "  'extensive',\n",
       "  'rearrangement',\n",
       "  'residues',\n",
       "  'ha2',\n",
       "  'low',\n",
       "  'ph',\n",
       "  'respect',\n",
       "  'relative',\n",
       "  'orientation',\n",
       "  'coil-coil',\n",
       "  'formation',\n",
       "  'loop-to-helix',\n",
       "  'helix-to-loop',\n",
       "  'transitions',\n",
       "  'bullough',\n",
       "  '1994',\n",
       "  'durrer',\n",
       "  '1996',\n",
       "  'plotch',\n",
       "  '1999',\n",
       "  'harrison',\n",
       "  '2008',\n",
       "  'initial',\n",
       "  'step',\n",
       "  'de-trimerization',\n",
       "  'membrane',\n",
       "  'distal',\n",
       "  'ha1',\n",
       "  'domain',\n",
       "  'triggered',\n",
       "  'low',\n",
       "  'ph',\n",
       "  'domain',\n",
       "  'remain',\n",
       "  'tethered',\n",
       "  'inner-core',\n",
       "  'triple-stranded',\n",
       "  'coiled',\n",
       "  'coil',\n",
       "  'structure',\n",
       "  'ha2',\n",
       "  'region',\n",
       "  'residues',\n",
       "  '28-43',\n",
       "  'bizebard',\n",
       "  '1995',\n",
       "  'step',\n",
       "  'driven',\n",
       "  'electrostatic',\n",
       "  'forces',\n",
       "  'positively',\n",
       "  'charged',\n",
       "  'ha1',\n",
       "  'favor',\n",
       "  'electrostatic',\n",
       "  'interaction',\n",
       "  'negatively',\n",
       "  'charged',\n",
       "  'ha2',\n",
       "  'neutral',\n",
       "  'ph',\n",
       "  'short',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '38-55',\n",
       "  'extended',\n",
       "  'loop',\n",
       "  'residues',\n",
       "  '56-75',\n",
       "  'n-terminal',\n",
       "  'region',\n",
       "  'ha2',\n",
       "  'become',\n",
       "  'extension',\n",
       "  'central',\n",
       "  'triple-stranded',\n",
       "  'coiled',\n",
       "  'coil',\n",
       "  'native',\n",
       "  'ha',\n",
       "  'form',\n",
       "  'long',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '38-104',\n",
       "  'relocating',\n",
       "  'fp',\n",
       "  '100',\n",
       "  'å',\n",
       "  'original',\n",
       "  'buried',\n",
       "  'position',\n",
       "  'time',\n",
       "  'middle',\n",
       "  'portion',\n",
       "  'residues',\n",
       "  '105-112',\n",
       "  'long',\n",
       "  'helix',\n",
       "  'ha2',\n",
       "  'converts',\n",
       "  'loop',\n",
       "  'allowing',\n",
       "  'second',\n",
       "  'half',\n",
       "  'long',\n",
       "  'helix',\n",
       "  'jackknife',\n",
       "  'back',\n",
       "  'lie',\n",
       "  'antiparallel',\n",
       "  'first',\n",
       "  'half',\n",
       "  'fig',\n",
       "  '2',\n",
       "  'resulting',\n",
       "  'formation',\n",
       "  'hairpin',\n",
       "  'structure',\n",
       "  'common',\n",
       "  'conformation',\n",
       "  'presented',\n",
       "  'class',\n",
       "  'transmembrane',\n",
       "  'proteins',\n",
       "  'enveloped',\n",
       "  'viruses',\n",
       "  'kielian',\n",
       "  'rey',\n",
       "  '2006',\n",
       "  'harrison',\n",
       "  '2008',\n",
       "  'interaction',\n",
       "  'fp',\n",
       "  'target',\n",
       "  'membrane',\n",
       "  'leads',\n",
       "  'extended',\n",
       "  'intermediate',\n",
       "  'bridges',\n",
       "  'viral',\n",
       "  'cell',\n",
       "  'membranes',\n",
       "  'intermediate',\n",
       "  'collapses',\n",
       "  'zipping',\n",
       "  'c-terminal',\n",
       "  'part',\n",
       "  'ectodomain',\n",
       "  'alongside',\n",
       "  'trimer-clustered',\n",
       "  'n-terminal',\n",
       "  'part',\n",
       "  'brings',\n",
       "  'two',\n",
       "  'membranes',\n",
       "  'close',\n",
       "  'proximity',\n",
       "  'resulting',\n",
       "  'formation',\n",
       "  'hemifusion',\n",
       "  'stalk',\n",
       "  'fusion',\n",
       "  'pore',\n",
       "  'opens',\n",
       "  'genetic',\n",
       "  'material',\n",
       "  'iav',\n",
       "  'released',\n",
       "  'host',\n",
       "  'cell',\n",
       "  'generate',\n",
       "  'new',\n",
       "  'virions',\n",
       "  'harrison',\n",
       "  '2008',\n",
       "  'cleavage',\n",
       "  'ha',\n",
       "  'precursor',\n",
       "  'ha0',\n",
       "  'ha1',\n",
       "  'ha2',\n",
       "  'subunits',\n",
       "  'host',\n",
       "  'cell',\n",
       "  'proteases',\n",
       "  'especially',\n",
       "  'airway',\n",
       "  'tryptase',\n",
       "  'clara',\n",
       "  'kido',\n",
       "  '1992',\n",
       "  'mini-plasmin',\n",
       "  'murakami',\n",
       "  '2001',\n",
       "  'ectopic',\n",
       "  'pancreatic',\n",
       "  'trypsin',\n",
       "  'le',\n",
       "  '2006',\n",
       "  'essential',\n",
       "  'subsequent',\n",
       "  'steps',\n",
       "  'viral',\n",
       "  'replication',\n",
       "  'liu',\n",
       "  '2009',\n",
       "  'okumura',\n",
       "  '2010',\n",
       "  'therefore',\n",
       "  'molecules',\n",
       "  'block',\n",
       "  'cleavage',\n",
       "  'ha0',\n",
       "  'may',\n",
       "  'inhibit',\n",
       "  'iav',\n",
       "  'infection',\n",
       "  'kido',\n",
       "  '2007',\n",
       "  'several',\n",
       "  'serine',\n",
       "  'protease',\n",
       "  'inhibitors',\n",
       "  'aprotinin',\n",
       "  '58-mer',\n",
       "  'peptide',\n",
       "  'zhirnov',\n",
       "  '1984',\n",
       "  'leupeptin',\n",
       "  'also',\n",
       "  'known',\n",
       "  'n-acetyl-l-leucyl-l-leucyl-largininal',\n",
       "  'tashiro',\n",
       "  '1987',\n",
       "  'schröder',\n",
       "  '1993',\n",
       "  'e-aminocaproic',\n",
       "  'acid',\n",
       "  'kido',\n",
       "  '1992',\n",
       "  'nafamostat',\n",
       "  'hosoya',\n",
       "  '1992',\n",
       "  'fig',\n",
       "  '3',\n",
       "  'pulmonary',\n",
       "  'surfactant',\n",
       "  'surface-active',\n",
       "  'lipoprotein',\n",
       "  'complex',\n",
       "  'phospholipoprotein',\n",
       "  'produced',\n",
       "  'type',\n",
       "  'ii',\n",
       "  'alveolar',\n",
       "  'cells',\n",
       "  'kido',\n",
       "  '1993',\n",
       "  'human',\n",
       "  'mucus',\n",
       "  'protease',\n",
       "  'inhibitor',\n",
       "  'beppu',\n",
       "  '1997',\n",
       "  'shown',\n",
       "  'reduce',\n",
       "  'ha',\n",
       "  'cleavage',\n",
       "  'iav',\n",
       "  'infection',\n",
       "  'cultures',\n",
       "  'animal',\n",
       "  'models',\n",
       "  'aprotinin',\n",
       "  'trasylol',\n",
       "  'bayer',\n",
       "  'bovine',\n",
       "  'pancreatic',\n",
       "  'trypsin',\n",
       "  'inhibitor',\n",
       "  'used',\n",
       "  'injection',\n",
       "  'drug',\n",
       "  'reduction',\n",
       "  'bleeding',\n",
       "  'complex',\n",
       "  'surgery',\n",
       "  'however',\n",
       "  'trasylol',\n",
       "  'withdrawn',\n",
       "  'market',\n",
       "  'may',\n",
       "  '2008',\n",
       "  'studies',\n",
       "  'suggested',\n",
       "  'use',\n",
       "  'increased',\n",
       "  'risk',\n",
       "  'complications',\n",
       "  'death',\n",
       "  'http',\n",
       "  '//www.trasylol.com/trasylol_11_05_07.pdf',\n",
       "  'several',\n",
       "  'fda',\n",
       "  'approved',\n",
       "  'pulmonary',\n",
       "  'surfactants',\n",
       "  'market',\n",
       "  'including',\n",
       "  'exosurf',\n",
       "  'curosurf',\n",
       "  'infasurf',\n",
       "  'survanta',\n",
       "  'increasing',\n",
       "  'pulmonary',\n",
       "  'compliance',\n",
       "  'preventing',\n",
       "  'infant',\n",
       "  'respiratory',\n",
       "  'distress',\n",
       "  'syndrome',\n",
       "  'irds',\n",
       "  'long',\n",
       "  'protein',\n",
       "  '&',\n",
       "  'cell',\n",
       "  'figure',\n",
       "  '2',\n",
       "  'diagrams',\n",
       "  'ph-induced',\n",
       "  'conformational',\n",
       "  'change',\n",
       "  'ha2',\n",
       "  'subunit',\n",
       "  'prefusion',\n",
       "  'postfusion',\n",
       "  'states',\n",
       "  'prefusion',\n",
       "  'state',\n",
       "  'neutral',\n",
       "  'ph',\n",
       "  'ha2',\n",
       "  'monomer',\n",
       "  'homotrimer',\n",
       "  'maintains',\n",
       "  'hairpin',\n",
       "  'structure',\n",
       "  'n-terminal',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '38-55',\n",
       "  'c-terminal',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '76-129',\n",
       "  'joined',\n",
       "  'extended',\n",
       "  'loop',\n",
       "  'residues',\n",
       "  '56-75',\n",
       "  'antiparallel',\n",
       "  'fashion',\n",
       "  'postfusion',\n",
       "  'state',\n",
       "  'acid',\n",
       "  'ph',\n",
       "  'ha2',\n",
       "  'monomer',\n",
       "  'still',\n",
       "  'retains',\n",
       "  'hairpin',\n",
       "  'structure',\n",
       "  'middle',\n",
       "  'portion',\n",
       "  'residues',\n",
       "  '105-112',\n",
       "  'original',\n",
       "  'long',\n",
       "  'helix',\n",
       "  'unfolds',\n",
       "  'form',\n",
       "  'reverse',\n",
       "  'turn',\n",
       "  'c-terminal',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '113-129',\n",
       "  'jack-knifes',\n",
       "  'back',\n",
       "  'lie',\n",
       "  'antiparallel',\n",
       "  'newly',\n",
       "  'formed',\n",
       "  'n-terminal',\n",
       "  'long',\n",
       "  'helix',\n",
       "  'residues',\n",
       "  '38-104',\n",
       "  'resulting',\n",
       "  'relocation',\n",
       "  'fp',\n",
       "  'dash',\n",
       "  'line',\n",
       "  '100',\n",
       "  'å',\n",
       "  'previously',\n",
       "  'buried',\n",
       "  'position',\n",
       "  'exposed',\n",
       "  'position',\n",
       "  'figure',\n",
       "  'prepared',\n",
       "  'using',\n",
       "  'pymol',\n",
       "  'program',\n",
       "  'delano',\n",
       "  '2005',\n",
       "  'http',\n",
       "  '//www.pymol.org',\n",
       "  'x-ray',\n",
       "  'crystal',\n",
       "  'structures',\n",
       "  'ha',\n",
       "  'neutral',\n",
       "  'ph',\n",
       "  'pdb',\n",
       "  'id',\n",
       "  '2viu',\n",
       "  'fleury',\n",
       "  '1998',\n",
       "  'low',\n",
       "  'ph',\n",
       "  'pdb',\n",
       "  'id',\n",
       "  '1htm',\n",
       "  'bullough',\n",
       "  '1994',\n",
       "  'downloaded',\n",
       "  'protein',\n",
       "  'data',\n",
       "  'bank',\n",
       "  'http',\n",
       "  '//www.rcsb.org/pdb/home/home.do',\n",
       "  'known',\n",
       "  'peptide',\n",
       "  'substrate',\n",
       "  'analog',\n",
       "  'inhibitor',\n",
       "  'dec-r-v-k-r-cmk',\n",
       "  'furin',\n",
       "  'could',\n",
       "  'block',\n",
       "  'h7',\n",
       "  'hpai',\n",
       "  'iav',\n",
       "  'replication',\n",
       "  'stieneke-gröber',\n",
       "  '1992',\n",
       "  'later',\n",
       "  'observed',\n",
       "  'dec-r-v-k-r-cmk',\n",
       "  'inhibits',\n",
       "  'furin',\n",
       "  'also',\n",
       "  'ubiquitous',\n",
       "  'type',\n",
       "  'ii',\n",
       "  'transmembrane',\n",
       "  'serine',\n",
       "  'proteases',\n",
       "  'mspl',\n",
       "  'splice',\n",
       "  'variant',\n",
       "  'tmprss13',\n",
       "  'trypsintype',\n",
       "  'protease',\n",
       "  'plasmin',\n",
       "  'okumura',\n",
       "  '1997',\n",
       "  'recently',\n",
       "  'kido',\n",
       "  'colleagues',\n",
       "  'reported',\n",
       "  'mspl/',\n",
       "  'tmprss13',\n",
       "  'may',\n",
       "  'also',\n",
       "  'serve',\n",
       "  'ha',\n",
       "  'processing',\n",
       "  'proteases',\n",
       "  'h5',\n",
       "  'h7',\n",
       "  'hpai',\n",
       "  'iavs',\n",
       "  'okumura',\n",
       "  '2010',\n",
       "  'therefore',\n",
       "  'natural',\n",
       "  'inhibitors',\n",
       "  'mspl',\n",
       "  'tmprss13',\n",
       "  'humans',\n",
       "  'may',\n",
       "  'also',\n",
       "  'used',\n",
       "  'leads',\n",
       "  'development',\n",
       "  'ha-based',\n",
       "  'anti-iav',\n",
       "  'drugs',\n",
       "  'ha-mediated',\n",
       "  'membrane',\n",
       "  'fusion',\n",
       "  'ha',\n",
       "  'activated',\n",
       "  'endosomes',\n",
       "  'acidic',\n",
       "  'ph',\n",
       "  'induction',\n",
       "  'irreversible',\n",
       "  ...]]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
